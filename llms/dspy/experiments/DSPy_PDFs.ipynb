{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1eccf3-2533-4786-9ef1-e0dbd01251a0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.cache/pypoetry/virtualenvs/llms-dspy-cWHDaHg3-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llms_dspy.utils import get_llm, get_chroma_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc9d622-f38e-4c54-925d-f839e0a60f13",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dspy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4d5b54-b169-4e41-b29c-dc5116633acf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is basically from https://github.com/weaviate/recipes/blob/main/integrations/dspy/Weaviate-Import.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee2a8c1a-82e6-4ee7-b136-1c0b2d3eb3a6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dsp.utils import deduplicate\n",
    "\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"Helpful information for answering the question.\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"A detailed answer that is supported by the context.\")\n",
    "\n",
    "class GenerateSearchQuery(dspy.Signature):\n",
    "    \"\"\"Write a search query that will help answer a complex question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"Contexts produced from previous search queries.\")\n",
    "    question = dspy.InputField(desc=\"The complex question we began with.\")\n",
    "    query = dspy.OutputField(desc=\"A search query that will help answer the question.\")\n",
    "\n",
    "class MultiHopRAG(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generate_question = dspy.ChainOfThought(GenerateSearchQuery)\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context_history = []\n",
    "        queries = []\n",
    "        \n",
    "        # Note to self, discuss how this differs from AutoGPT\n",
    "        context = self.retrieve(question).passages\n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_question(context=context, question=question).query\n",
    "            queries.append(query) # For inspection\n",
    "            passages = self.retrieve(query).passages\n",
    "            context_history.append(passages)\n",
    "            context = deduplicate(context + passages)\n",
    "    \n",
    "        pred = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(answer=pred.answer, queries=queries, context_history=context_history, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c8e6dd-306b-4b89-91b2-ee517afe58c7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class ProgramWrapper:\n",
    "    def __init__(self, name, program):\n",
    "        self.name = name\n",
    "        self.uncompiled = program()\n",
    "        self.uncomplied_score = 0.0\n",
    "        self.compiled = None\n",
    "        self.compiled_score = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19aed708-8b1b-4822-aa2a-abcffbd16d3b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chatgpt = get_llm(\"openai\", openai_key_path=\"~/.keys/openai_key.txt\")\n",
    "\n",
    "chroma_retriever = get_chroma_retriever(\"../chroma_db\", \"pdf_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24dec69c-7200-429e-ad19-7e20acf7e0d6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The BART model, which stands for Bayesian Additive Regression Trees, is a statistical model used for regression analysis. It combines the flexibility of regression trees with the Bayesian framework to provide a powerful tool for modeling complex relationships in data. BART models are particularly useful for handling high-dimensional data and capturing non-linear relationships between variables. They have been successfully applied in various fields such as finance, healthcare, and environmental science.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt(\"what is BART model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f456ded5-b272-455a-a274-ec56008c1d83",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=chatgpt, rm=chroma_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5bac73-3ab1-43fc-b785-73287d3489e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rag = MultiHopRAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57aeabd8-4af0-4ed1-b612-0c11863ff299",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='BART is a type of neural network known as a sequence-to-sequence model.',\n",
       "    queries=['\"BART neural network architecture\"', '\"BART neural network architecture and design\"'],\n",
       "    context_history=[['Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\\nKrueger, Kevin Button, Matthew Knight, Benjamin\\nChess, and John Schulman. 2022. Webgpt: Browser-\\nassisted question-answering with human feedback.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow instruc-\\ntions with human feedback. Advances in Neural\\nInformation Processing Systems , 35:27730–27744.\\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\\nTool augmented language models.\\nJohn V Pavlik. 2023. Collaborating with chatgpt: Con-\\nsidering the implications of generative artificial intel-\\nligence for journalism and media education. Journal-\\nism & Mass Communication Educator , 78(1):84–93.\\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari,\\nGustavo Soares, Christopher Meek, and Sumit Gul-\\nwani. 2022. Synchromesh: Reliable code generation\\nfrom pre-trained language models. In International\\nConference on Learning Representations .\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah Smith, and Mike Lewis. 2023. Measuring and\\nnarrowing the compositionality gap in language mod-\\nels. In Findings of the Association for Computational\\nLinguistics: EMNLP 2023 , pages 5687–5711, Singa-\\npore. Association for Computational Linguistics.\\nPeng Qi, Yuhao Zhang, and Christopher D. Manning.\\n2020. Stay hungry, stay focused: Generating infor-\\nmative and specific questions in information-seeking\\nconversations. In Findings of the Association for\\nComputational Linguistics: EMNLP 2020 , pages 25–\\n40, Online. Association for Computational Linguis-\\ntics.', 'SELF-DISCOVER : Large Language Models Self-Compose Reasoning Structures\\nAcknowledgement\\nWe thank Andrew Dai and Adams Yu of Google DeepMind\\nfor their insightful feedback on this paper.\\nReferences\\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\\nD., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\\nZ., et al. Palm 2 technical report. arXiv preprint\\narXiv:2305.10403 , 2023.\\nBesta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gi-\\naninazzi, L., Gajda, J., Lehmann, T., Podstawski, M.,\\nNiewiadomski, H., Nyczyk, P., et al. Graph of thoughts:\\nSolving elaborate problems with large language models.\\narXiv preprint arXiv:2308.09687 , 2023.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems , 33:\\n1877–1901, 2020.\\nChen, J., Pan, X., Yu, D., Song, K., Wang, X., Yu, D.,\\nand Chen, J. Skills-in-context prompting: Unlocking\\ncompositionality in large language models. arXiv preprint\\narXiv:2308.00304 , 2023.\\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\\nof thoughts prompting: Disentangling computation from\\nreasoning for numerical reasoning tasks. arXiv preprint\\narXiv:2211.12588 , 2022.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\\nGehrmann, S., et al. Palm: Scaling language modeling\\nwith pathways. arXiv preprint arXiv:2204.02311 , 2022.\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\\nFedus, W., Li, Y ., Wang, X., Dehghani, M., Brahma,\\nS., et al. Scaling instruction-finetuned language models.\\narXiv preprint arXiv:2210.11416 , 2022.\\nCobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H.,\\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\\nR., et al. Training verifiers to solve math word problems.\\narXiv preprint arXiv:2110.14168 , 2021.\\nDrozdov, A., Schärli, N., Akyürek, E., Scales, N., Song,\\nX., Chen, X., Bousquet, O., and Zhou, D. Composi-\\ntional semantic parsing with large language models. arXiv\\npreprint arXiv:2209.15003 , 2022.\\nFernando, C., Banarse, D., Michalewski, H., Osindero,\\nS., and Rocktäschel, T. Promptbreeder: Self-referential\\nself-improvement via prompt evolution. arXiv preprint\\narXiv:2309.16797 , 2023.Gao, C., Jiang, H., Cai, D., Shi, S., and Lam, W. Strategyllm:\\nLarge language models as strategy generators, executors,\\noptimizers, and evaluators for problem solving. arXiv\\npreprint arXiv:2311.08803 , 2023a.\\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang,\\nY ., Callan, J., and Neubig, G. Pal: Program-aided lan-\\nguage models. In International Conference on Machine\\nLearning , pp. 10764–10799. PMLR, 2023b.\\nHao, S., Gu, Y ., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z.,\\nand Hu, Z. Reasoning with language model is planning\\nwith world model. arXiv preprint arXiv:2305.14992 ,\\n2023.\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\\nS., Tang, E., Song, D., and Steinhardt, J. Measuring\\nmathematical problem solving with the math dataset. Sort,\\n2(4):0–6.\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\\nematical problem solving with the math dataset, 2021.\\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\\nClark, P., and Sabharwal, A. Decomposed prompting:\\nA modular approach for solving complex tasks. In The\\nEleventh International Conference on Learning Repre-\\nsentations , 2022.\\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\\nY . Large language models are zero-shot reasoners. Ad-\\nvances in neural information processing systems , 35:\\n22199–22213, 2022.\\nKuznia, K., Mishra, S., Parmar, M., and Baral, C. Less is\\nmore: Summary of long instructions is better for program\\nsynthesis. In Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing , pp.\\n4532–4552, 2022.\\nLiu, T., Guo, Q., Yang, Y ., Hu, X., Zhang, Y ., Qiu, X.,', 'Wang, X., Wei, J., Schuurmans, D., Le, Q. V ., Chi,\\nE. H., Narang, S., Chowdhery, A., and Zhou, D. Self-\\nconsistency improves chain of thought reasoning in lan-\\nguage models. In The Eleventh International Conference\\non Learning Representations , 2022.\\nWei, J., Bosma, M., Zhao, V ., Guu, K., Yu, A. W., Lester,\\nB., Du, N., Dai, A. M., and Le, Q. V . Finetuned lan-\\nguage models are zero-shot learners. In International\\nConference on Learning Representations , 2021.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\\nChi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought\\nprompting elicits reasoning in large language models.\\nAdvances in Neural Information Processing Systems , 35:\\n24824–24837, 2022.\\nYang, C., Wang, X., Lu, Y ., Liu, H., Le, Q. V ., Zhou, D., and\\nChen, X. Large language models as optimizers. arXiv\\npreprint arXiv:2309.03409 , 2023.\\n10'], ['Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\\nKrueger, Kevin Button, Matthew Knight, Benjamin\\nChess, and John Schulman. 2022. Webgpt: Browser-\\nassisted question-answering with human feedback.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow instruc-\\ntions with human feedback. Advances in Neural\\nInformation Processing Systems , 35:27730–27744.\\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\\nTool augmented language models.\\nJohn V Pavlik. 2023. Collaborating with chatgpt: Con-\\nsidering the implications of generative artificial intel-\\nligence for journalism and media education. Journal-\\nism & Mass Communication Educator , 78(1):84–93.\\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari,\\nGustavo Soares, Christopher Meek, and Sumit Gul-\\nwani. 2022. Synchromesh: Reliable code generation\\nfrom pre-trained language models. In International\\nConference on Learning Representations .\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah Smith, and Mike Lewis. 2023. Measuring and\\nnarrowing the compositionality gap in language mod-\\nels. In Findings of the Association for Computational\\nLinguistics: EMNLP 2023 , pages 5687–5711, Singa-\\npore. Association for Computational Linguistics.\\nPeng Qi, Yuhao Zhang, and Christopher D. Manning.\\n2020. Stay hungry, stay focused: Generating infor-\\nmative and specific questions in information-seeking\\nconversations. In Findings of the Association for\\nComputational Linguistics: EMNLP 2020 , pages 25–\\n40, Online. Association for Computational Linguis-\\ntics.', 'Wang, X., Wei, J., Schuurmans, D., Le, Q. V ., Chi,\\nE. H., Narang, S., Chowdhery, A., and Zhou, D. Self-\\nconsistency improves chain of thought reasoning in lan-\\nguage models. In The Eleventh International Conference\\non Learning Representations , 2022.\\nWei, J., Bosma, M., Zhao, V ., Guu, K., Yu, A. W., Lester,\\nB., Du, N., Dai, A. M., and Le, Q. V . Finetuned lan-\\nguage models are zero-shot learners. In International\\nConference on Learning Representations , 2021.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\\nChi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought\\nprompting elicits reasoning in large language models.\\nAdvances in Neural Information Processing Systems , 35:\\n24824–24837, 2022.\\nYang, C., Wang, X., Lu, Y ., Liu, H., Le, Q. V ., Zhou, D., and\\nChen, X. Large language models as optimizers. arXiv\\npreprint arXiv:2309.03409 , 2023.\\n10', 'SELF-DISCOVER : Large Language Models Self-Compose Reasoning Structures\\nAcknowledgement\\nWe thank Andrew Dai and Adams Yu of Google DeepMind\\nfor their insightful feedback on this paper.\\nReferences\\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\\nD., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\\nZ., et al. Palm 2 technical report. arXiv preprint\\narXiv:2305.10403 , 2023.\\nBesta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gi-\\naninazzi, L., Gajda, J., Lehmann, T., Podstawski, M.,\\nNiewiadomski, H., Nyczyk, P., et al. Graph of thoughts:\\nSolving elaborate problems with large language models.\\narXiv preprint arXiv:2308.09687 , 2023.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems , 33:\\n1877–1901, 2020.\\nChen, J., Pan, X., Yu, D., Song, K., Wang, X., Yu, D.,\\nand Chen, J. Skills-in-context prompting: Unlocking\\ncompositionality in large language models. arXiv preprint\\narXiv:2308.00304 , 2023.\\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\\nof thoughts prompting: Disentangling computation from\\nreasoning for numerical reasoning tasks. arXiv preprint\\narXiv:2211.12588 , 2022.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\\nGehrmann, S., et al. Palm: Scaling language modeling\\nwith pathways. arXiv preprint arXiv:2204.02311 , 2022.\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\\nFedus, W., Li, Y ., Wang, X., Dehghani, M., Brahma,\\nS., et al. Scaling instruction-finetuned language models.\\narXiv preprint arXiv:2210.11416 , 2022.\\nCobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H.,\\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\\nR., et al. Training verifiers to solve math word problems.\\narXiv preprint arXiv:2110.14168 , 2021.\\nDrozdov, A., Schärli, N., Akyürek, E., Scales, N., Song,\\nX., Chen, X., Bousquet, O., and Zhou, D. Composi-\\ntional semantic parsing with large language models. arXiv\\npreprint arXiv:2209.15003 , 2022.\\nFernando, C., Banarse, D., Michalewski, H., Osindero,\\nS., and Rocktäschel, T. Promptbreeder: Self-referential\\nself-improvement via prompt evolution. arXiv preprint\\narXiv:2309.16797 , 2023.Gao, C., Jiang, H., Cai, D., Shi, S., and Lam, W. Strategyllm:\\nLarge language models as strategy generators, executors,\\noptimizers, and evaluators for problem solving. arXiv\\npreprint arXiv:2311.08803 , 2023a.\\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang,\\nY ., Callan, J., and Neubig, G. Pal: Program-aided lan-\\nguage models. In International Conference on Machine\\nLearning , pp. 10764–10799. PMLR, 2023b.\\nHao, S., Gu, Y ., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z.,\\nand Hu, Z. Reasoning with language model is planning\\nwith world model. arXiv preprint arXiv:2305.14992 ,\\n2023.\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\\nS., Tang, E., Song, D., and Steinhardt, J. Measuring\\nmathematical problem solving with the math dataset. Sort,\\n2(4):0–6.\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\\nematical problem solving with the math dataset, 2021.\\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\\nClark, P., and Sabharwal, A. Decomposed prompting:\\nA modular approach for solving complex tasks. In The\\nEleventh International Conference on Learning Repre-\\nsentations , 2022.\\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\\nY . Large language models are zero-shot reasoners. Ad-\\nvances in neural information processing systems , 35:\\n22199–22213, 2022.\\nKuznia, K., Mishra, S., Parmar, M., and Baral, C. Less is\\nmore: Summary of long instructions is better for program\\nsynthesis. In Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing , pp.\\n4532–4552, 2022.\\nLiu, T., Guo, Q., Yang, Y ., Hu, X., Zhang, Y ., Qiu, X.,']],\n",
       "    question='What kind of neural network is BART?'\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"What kind of neural network is BART?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "797e39fd-8193-46c3-a0c9-320e65ce3549",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'long_text': 'Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\\nKrueger, Kevin Button, Matthew Knight, Benjamin\\nChess, and John Schulman. 2022. Webgpt: Browser-\\nassisted question-answering with human feedback.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow instruc-\\ntions with human feedback. Advances in Neural\\nInformation Processing Systems , 35:27730–27744.\\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\\nTool augmented language models.\\nJohn V Pavlik. 2023. Collaborating with chatgpt: Con-\\nsidering the implications of generative artificial intel-\\nligence for journalism and media education. Journal-\\nism & Mass Communication Educator , 78(1):84–93.\\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari,\\nGustavo Soares, Christopher Meek, and Sumit Gul-\\nwani. 2022. Synchromesh: Reliable code generation\\nfrom pre-trained language models. In International\\nConference on Learning Representations .\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah Smith, and Mike Lewis. 2023. Measuring and\\nnarrowing the compositionality gap in language mod-\\nels. In Findings of the Association for Computational\\nLinguistics: EMNLP 2023 , pages 5687–5711, Singa-\\npore. Association for Computational Linguistics.\\nPeng Qi, Yuhao Zhang, and Christopher D. Manning.\\n2020. Stay hungry, stay focused: Generating infor-\\nmative and specific questions in information-seeking\\nconversations. In Findings of the Association for\\nComputational Linguistics: EMNLP 2020 , pages 25–\\n40, Online. Association for Computational Linguis-\\ntics.'},\n",
       " {'long_text': 'Published as a conference paper at ICLR 2024\\nQuestion Answering with Long Input Texts, Yes! In Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies , pp. 5336–5358, Seattle, United States, July 2022. Association for Computational\\nLinguistics. URL https://aclanthology.org/2022.naacl-main.391 .\\nFabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\\nand Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 ,\\n2019. URL https://arxiv.org/abs/1909.01066 .\\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\\nMethods, Analysis & Insights from Training Gopher. arXiv preprint arXiv:2112.11446 , 2021.\\nURL https://arxiv.org/abs/2112.11446 .\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-\\nBrown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint\\narXiv:2302.00083 , 2023. URL https://arxiv.org/abs/2302.00083 .\\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP) , pp. 3982–3992, Hong Kong, China, November 2019. Association for Com-\\nputational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/\\nD19-1410 .\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How Much Knowledge Can You Pack Into\\nthe Parameters of a Language Model? In Proceedings of the 2020 Conference on Empir-\\nical Methods in Natural Language Processing (EMNLP) , pp. 5418–5426, Online, November\\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL\\nhttps://aclanthology.org/2020.emnlp-main.437 .\\nStephen Robertson, Hugo Zaragoza, et al. The Probabilistic Relevance Framework: BM25 and\\nBeyond. Foundations and Trends in Information Retrieval , 3(4):333–389, 2009. URL https:\\n//doi.org/10.1561/1500000019 .\\nStephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,\\net al. Okapi at TREC-3. Nist Special Publication Sp , 109:109, 1995. URL https://www.\\nmicrosoft.com/en-us/research/publication/okapi-at-trec-3/ .\\nDevendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\\nZaheer. Questions are all you need to train a dense passage retriever. Transactions of the As-\\nsociation for Computational Linguistics , 11:600–616, 2023. doi: 10.1162/tacl a00564. URL\\nhttps://aclanthology.org/2023.tacl-1.35 .\\nGideon Schwarz. Estimating the Dimension of a Model. The annals of statistics , pp. 461–464,\\n1978. URL https://projecteuclid.org/journals/annals-of-statistics/\\nvolume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/\\naos/1176344136.full .\\nKaren Sp ¨arck Jones. A Statistical Interpretation of Term Specificity and its Application in Re-\\ntrieval. Journal of documentation , 28(1):11–21, 1972. URL https://doi.org/10.1108/\\neb026526 .\\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\\nmodels actually use long-range context? In Marie-Francine Moens, Xuanjing Huang, Lucia\\nSpecia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing , pp. 807–822, Online and Punta Cana, Dominican Republic,\\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\\n62. URL https://aclanthology.org/2021.emnlp-main.62 .\\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\\nmodels. arXiv preprint arXiv:2210.01296 , 2022. URL https://arxiv.org/abs/2210.\\n01296 .\\n14'},\n",
       " {'long_text': 'Published as a conference paper at ICLR 2024\\nMandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and\\nYinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the\\nAssociation for Computational Linguistics: NAACL 2022 , pp. 724–736, Seattle, United States,\\nJuly 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55.\\nURLhttps://aclanthology.org/2022.findings-naacl.55 .\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval Augmented\\nLanguage Model Pre-Training. In International conference on machine learning , pp. 3929–3938.\\nPMLR, 2020. URL https://doi.org/10.48550/arXiv.2002.08909 .\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022. URL\\nhttps://arxiv.org/abs/2203.15556 .\\nGautier Izacard and Edouard Grave. Distilling Knowledge from Reader to Retriever for Ques-\\ntion Answering, 2022. URL https://arxiv.org/abs/2012.04584 . arXiv preprint\\narXiv:2012.04584.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re-\\ntrieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022. URL https:\\n//arxiv.org/abs/2208.03299 .\\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\\nmodels know? Transactions of the Association for Computational Linguistics , 8:423–438, 2020.\\nURL https://arxiv.org/abs/1911.12543 .\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. Billion-Scale Similarity Search with GPUs. IEEE\\nTransactions on Big Data , 7(3):535–547, 2019. URL https://arxiv.org/abs/1702.\\n08734 .\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large Language\\nModels struggle to learn Long-Tail Knowledge. In International Conference on Machine Learn-\\ning, pp. 15696–15707. PMLR, 2023. URL https://proceedings.mlr.press/v202/\\nkandpal23a/kandpal23a.pdf .\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In\\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP) , pp. 6769–6781, Online, November 2020. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.\\nemnlp-main.550 .\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\\nHannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system.\\nInFindings of the Association for Computational Linguistics: EMNLP 2020 , pp. 1896–1907,\\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\\nfindings-emnlp.171. URL https://aclanthology.org/2020.findings-emnlp.\\n171.\\nOmar Khattab and Matei Zaharia. ColBERT: Efficient and effective passage search via con-\\ntextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR\\nconference on research and development in Information Retrieval , pp. 39–48, 2020. URL\\nhttps://arxiv.org/abs/2004.12832 .\\nTom´aˇs Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G ´abor Melis,\\nand Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. Transactions\\nof the Association for Computational Linguistics , 6:317–328, 2018. URL https://arxiv.\\norg/abs/1712.07040 .\\n12'},\n",
       " {'long_text': 'Published as a conference paper at ICLR 2024\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¨aschel, et al. Retrieval-Augmented Gener-\\nation for Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems ,\\n33:9459–9474, 2020. URL https://doi.org/10.48550/arXiv.2005.11401 .\\nJerry Liu. LlamaIndex, 2022. URL https://github.com/jerryjliu/llama_index .\\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\\nPercy Liang. Lost in the middle: How language models use long contexts. arXiv preprint\\narXiv:2307.03172 , 2023. URL https://arxiv.org/abs/2307.03172 .\\nYe Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu. Dense\\nhierarchical retrieval for open-domain question answering. In Marie-Francine Moens, Xuanjing\\nHuang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Compu-\\ntational Linguistics: EMNLP 2021 , pp. 188–200, Punta Cana, Dominican Republic, Novem-\\nber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.19.\\nURL https://aclanthology.org/2021.findings-emnlp.19 .\\nLeland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation\\nand Projection for Dimension Reduction, 2018. URL https://arxiv.org/abs/1802.\\n03426 . arXiv preprint arXiv:1802.03426.\\nSewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. Joint\\npassage ranking for diverse multi-answer retrieval. In Marie-Francine Moens, Xuanjing Huang,\\nLucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical\\nMethods in Natural Language Processing , pp. 6997–7008, Online and Punta Cana, Dominican\\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\\nemnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560 .\\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke\\nZettlemoyer. Nonparametric masked language modeling. In Findings of the Association for\\nComputational Linguistics: ACL 2023 , pp. 2097–2118, Toronto, Canada, July 2023. Associ-\\nation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL https:\\n//aclanthology.org/2023.findings-acl.132 .\\nEric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn.\\nMemory-based model editing at scale. In International Conference on Machine Learning ,\\npp. 15817–15831. PMLR, 2022. URL https://proceedings.mlr.press/v162/\\nmitchell22a/mitchell22a.pdf .\\nXiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui\\nSu. Frustratingly hard evidence retrieval for QA over books. In Proceedings of the First Joint\\nWorkshop on Narrative Understanding, Storylines, and Events , pp. 108–113, Online, July 2020.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.nuse-1.13. URL https:\\n//aclanthology.org/2020.nuse-1.13 .\\nInderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikr-\\nishna Karanam, and Sumit Shekhar. A neural CRF-based hierarchical approach for lin-\\near text segmentation. In Findings of the Association for Computational Linguistics: EACL\\n2023 , pp. 883–893, Dubrovnik, Croatia, May 2023. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2023.findings-eacl.65. URL https://aclanthology.org/2023.\\nfindings-eacl.65 .\\nBenjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. A controllable qa-\\nbased framework for decontextualization. arXiv preprint arXiv:2305.14772 , 2023. URL https:\\n//arxiv.org/pdf/2305.14772.pdf .\\nOpenAI. GPT-4 Technical Report. ArXiv , abs/2303.08774, 2023. URL https://arxiv.org/\\nabs/2303.08774 .\\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,\\nVishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY:\\n13'},\n",
       " {'long_text': 'SELF-DISCOVER : Large Language Models Self-Compose Reasoning Structures\\nAcknowledgement\\nWe thank Andrew Dai and Adams Yu of Google DeepMind\\nfor their insightful feedback on this paper.\\nReferences\\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\\nD., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\\nZ., et al. Palm 2 technical report. arXiv preprint\\narXiv:2305.10403 , 2023.\\nBesta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gi-\\naninazzi, L., Gajda, J., Lehmann, T., Podstawski, M.,\\nNiewiadomski, H., Nyczyk, P., et al. Graph of thoughts:\\nSolving elaborate problems with large language models.\\narXiv preprint arXiv:2308.09687 , 2023.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems , 33:\\n1877–1901, 2020.\\nChen, J., Pan, X., Yu, D., Song, K., Wang, X., Yu, D.,\\nand Chen, J. Skills-in-context prompting: Unlocking\\ncompositionality in large language models. arXiv preprint\\narXiv:2308.00304 , 2023.\\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\\nof thoughts prompting: Disentangling computation from\\nreasoning for numerical reasoning tasks. arXiv preprint\\narXiv:2211.12588 , 2022.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\\nGehrmann, S., et al. Palm: Scaling language modeling\\nwith pathways. arXiv preprint arXiv:2204.02311 , 2022.\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\\nFedus, W., Li, Y ., Wang, X., Dehghani, M., Brahma,\\nS., et al. Scaling instruction-finetuned language models.\\narXiv preprint arXiv:2210.11416 , 2022.\\nCobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H.,\\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\\nR., et al. Training verifiers to solve math word problems.\\narXiv preprint arXiv:2110.14168 , 2021.\\nDrozdov, A., Schärli, N., Akyürek, E., Scales, N., Song,\\nX., Chen, X., Bousquet, O., and Zhou, D. Composi-\\ntional semantic parsing with large language models. arXiv\\npreprint arXiv:2209.15003 , 2022.\\nFernando, C., Banarse, D., Michalewski, H., Osindero,\\nS., and Rocktäschel, T. Promptbreeder: Self-referential\\nself-improvement via prompt evolution. arXiv preprint\\narXiv:2309.16797 , 2023.Gao, C., Jiang, H., Cai, D., Shi, S., and Lam, W. Strategyllm:\\nLarge language models as strategy generators, executors,\\noptimizers, and evaluators for problem solving. arXiv\\npreprint arXiv:2311.08803 , 2023a.\\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang,\\nY ., Callan, J., and Neubig, G. Pal: Program-aided lan-\\nguage models. In International Conference on Machine\\nLearning , pp. 10764–10799. PMLR, 2023b.\\nHao, S., Gu, Y ., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z.,\\nand Hu, Z. Reasoning with language model is planning\\nwith world model. arXiv preprint arXiv:2305.14992 ,\\n2023.\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\\nS., Tang, E., Song, D., and Steinhardt, J. Measuring\\nmathematical problem solving with the math dataset. Sort,\\n2(4):0–6.\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\\nematical problem solving with the math dataset, 2021.\\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\\nClark, P., and Sabharwal, A. Decomposed prompting:\\nA modular approach for solving complex tasks. In The\\nEleventh International Conference on Learning Repre-\\nsentations , 2022.\\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\\nY . Large language models are zero-shot reasoners. Ad-\\nvances in neural information processing systems , 35:\\n22199–22213, 2022.\\nKuznia, K., Mishra, S., Parmar, M., and Baral, C. Less is\\nmore: Summary of long instructions is better for program\\nsynthesis. In Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing , pp.\\n4532–4552, 2022.\\nLiu, T., Guo, Q., Yang, Y ., Hu, X., Zhang, Y ., Qiu, X.,'},\n",
       " {'long_text': 'Computational Linguistics (Demonstrations) , pages\\n54–59, Minneapolis, Minnesota. Association for\\nComputational Linguistics.\\nMohammad Aliannejadi, Hamed Zamani, Fabio\\nCrestani, and W Bruce Croft. 2019. Asking clari-\\nfying questions in open-domain information-seeking\\nconversations. In Proceedings of the 42nd interna-\\ntional acm sigir conference on research and develop-\\nment in information retrieval , pages 475–484.\\nNishant Balepur, Jie Huang, and Kevin Chang. 2023.\\nExpository text generation: Imitate, retrieve, para-\\nphrase. In Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Process-\\ning, pages 11896–11919, Singapore. Association for\\nComputational Linguistics.\\nSiddhartha Banerjee and Prasenjit Mitra. 2015.\\nWikiKreator: Improving Wikipedia stubs automat-\\nically. In Proceedings of the 53rd Annual Meet-\\ning of the Association for Computational Linguis-\\ntics and the 7th International Joint Conference on\\nNatural Language Processing (Volume 1: Long Pa-\\npers) , pages 867–877, Beijing, China. Association\\nfor Computational Linguistics.\\nBernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aha-\\nroni, Daniel Andor, Livio Baldini Soares, Massimil-\\niano Ciaramita, Jacob Eisenstein, Kuzman Ganchev,\\nJonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma,\\nJianmo Ni, Lierni Sestorain Saralegui, Tal Schus-\\nter, William W. Cohen, Michael Collins, Dipanjan\\nDas, Donald Metzler, Slav Petrov, and Kellie Webster.\\n2023. Attributed question answering: Evaluation and\\nmodeling for attributed large language models.\\nWayne C Booth, Gregory G Colomb, and Joseph M\\nWilliams. 2003. The craft of research . University of\\nChicago press.\\nLaura Dietz and John Foley. 2019. Trec car y3: Com-\\nplex answer retrieval overview. In Proceedings of\\nText REtrieval Conference (TREC) .\\nChristina S Doyle. 1994. Information literacy in an\\ninformation society: A concept for the information\\nage. Diane Publishing.\\nAnn-Marie Eriksson and Åsa Mäkitalo. 2015. Supervi-\\nsion at the outline stage: Introducing and encounter-\\ning issues of sustainable development through aca-\\ndemic writing assignments. Text & Talk , 35(2):123–\\n153.\\nAngela Fan and Claire Gardent. 2022. Generating bi-\\nographies on Wikipedia: The impact of gender bias\\non the retrieval-based generation of women biogra-\\nphies. In Proceedings of the 60th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers) , pages 8561–8576, Dublin,\\nIreland. Association for Computational Linguistics.\\nXiaocheng Feng, Ming Liu, Jiahao Liu, Bing Qin, Yibo\\nSun, and Ting Liu. 2018. Topic-to-essay generation\\nwith neural networks. In IJCAI , pages 4078–4084.Tira Nur Fitria. 2023. Artificial intelligence (ai) tech-\\nnology in openai chatgpt application: A review of\\nchatgpt in writing english essay. In ELT Forum: Jour-\\nnal of English Language Teaching , volume 12, pages\\n44–58.\\nPasi Fränti and Radu Mariescu-Istodor. 2023. Soft preci-\\nsion and recall. Pattern Recognition Letters , 167:115–\\n121.\\nR Edward Freeman, Jeffrey S Harrison, Andrew C\\nWicks, Bidhan L Parmar, and Simone De Colle. 2010.\\nStakeholder theory: The state of the art.\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\\n2023. Enabling large language models to generate\\ntext with citations. In Proceedings of the 2023 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 6465–6488, Singapore. Associa-\\ntion for Computational Linguistics.\\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\\nZhangyin Feng, Haotian Wang, Qianglong Chen,\\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting\\nLiu. 2023. A survey on hallucination in large lan-\\nguage models: Principles, taxonomy, challenges, and\\nopen questions.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\\nYu, Armand Joulin, Sebastian Riedel, and Edouard\\nGrave. 2023. Atlas: Few-shot learning with retrieval\\naugmented language models. Journal of Machine\\nLearning Research , 24(251):1–43.\\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-'},\n",
       " {'long_text': 'synthesis. In Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing , pp.\\n4532–4552, 2022.\\nLiu, T., Guo, Q., Yang, Y ., Hu, X., Zhang, Y ., Qiu, X.,\\nand Zhang, Z. Plan, verify and switch: Integrated\\nreasoning with diverse x-of-thoughts. arXiv preprint\\narXiv:2310.14628 , 2023.\\nMishra, S. and Nouri, E. HELP ME THINK: A simple\\nprompting strategy for non-experts to create customized\\ncontent with models. In Rogers, A., Boyd-Graber, J.,\\nand Okazaki, N. (eds.), Findings of the Association for\\nComputational Linguistics: ACL 2023 , pp. 11834–11890,\\nToronto, Canada, July 2023. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2023.findings-acl.\\n751. URL https://aclanthology.org/2023.\\nfindings-acl.751 .\\n9'},\n",
       " {'long_text': 'Grave. 2023. Atlas: Few-shot learning with retrieval\\naugmented language models. Journal of Machine\\nLearning Research , 24(251):1–43.\\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\\nlaume Lample, Lucile Saulnier, et al. 2023a. Mistral\\n7b.arXiv preprint arXiv:2310.06825 .\\nZhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun,\\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\\nCallan, and Graham Neubig. 2023b. Active retrieval\\naugmented generation. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Lan-\\nguage Processing , pages 7969–7992, Singapore. As-\\nsociation for Computational Linguistics.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\\nWallace, and Colin Raffel. 2023. Large language\\nmodels struggle to learn long-tail knowledge. In In-\\nternational Conference on Machine Learning , pages\\n15696–15707. PMLR.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa\\nLi, David Hall, Percy Liang, Christopher Potts,\\nand Matei Zaharia. 2022. Demonstrate-search-\\npredict: Composing retrieval and language mod-\\nels for knowledge-intensive NLP. arXiv preprint\\narXiv:2212.14024 .\\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari,\\nZhiyuan Zhang, Keshav Santhanam, Sri Vard-\\nhamanan, Saiful Haq, Ashutosh Sharma, Thomas T.'},\n",
       " {'long_text': 'Wang, X., Wei, J., Schuurmans, D., Le, Q. V ., Chi,\\nE. H., Narang, S., Chowdhery, A., and Zhou, D. Self-\\nconsistency improves chain of thought reasoning in lan-\\nguage models. In The Eleventh International Conference\\non Learning Representations , 2022.\\nWei, J., Bosma, M., Zhao, V ., Guu, K., Yu, A. W., Lester,\\nB., Du, N., Dai, A. M., and Le, Q. V . Finetuned lan-\\nguage models are zero-shot learners. In International\\nConference on Learning Representations , 2021.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\\nChi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought\\nprompting elicits reasoning in large language models.\\nAdvances in Neural Information Processing Systems , 35:\\n24824–24837, 2022.\\nYang, C., Wang, X., Lu, Y ., Liu, H., Le, Q. V ., Zhou, D., and\\nChen, X. Large language models as optimizers. arXiv\\npreprint arXiv:2309.03409 , 2023.\\n10'},\n",
       " {'long_text': 'SELF-DISCOVER : Large Language Models Self-Compose Reasoning Structures\\nMishra, S., Finlayson, M., Lu, P., Tang, L., Welleck, S.,\\nBaral, C., Rajpurohit, T., Tafjord, O., Sabharwal, A.,\\nClark, P., et al. Lila: A unified benchmark for mathemati-\\ncal reasoning. In Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing , pp.\\n5807–5832, 2022a.\\nMishra, S., Khashabi, D., Baral, C., Choi, Y ., and Hajishirzi,\\nH. Reframing instructional prompts to gptk’s language.\\nInFindings of the Association for Computational Linguis-\\ntics: ACL 2022 , pp. 589–612, 2022b.\\nMishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Cross-\\ntask generalization via natural language crowdsourcing\\ninstructions. In Proceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Volume\\n1: Long Papers) , pp. 3470–3487, 2022c.\\nNewell, A., Shaw, J. C., and Simon, H. A. Elements of a\\ntheory of human problem solving. Psychological review ,\\n65(3):151, 1958.\\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H.,\\nAustin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma,\\nM., Luan, D., et al. Show your work: Scratchpads for\\nintermediate computation with language models. arXiv\\npreprint arXiv:2112.00114 , 2021.\\nOpenAI. Chatgpt: Optimizing language models for dia-\\nlogue, 2022. URL https://openai.com/blog/\\nchatgpt/ .\\nOpenAI. Json generation mode, 2023a. URL\\nhttps://platform.openai.com/docs/\\nguides/text-generation/json-mode .\\nOpenAI, R. Gpt-4 technical report. arXiv , pp. 2303–08774,\\n2023b.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\nwith human feedback. Advances in Neural Information\\nProcessing Systems , 35:27730–27744, 2022.\\nPatel, P., Mishra, S., Parmar, M., and Baral, C. Is a ques-\\ntion decomposition unit all we need? In Proceedings of\\nthe 2022 Conference on Empirical Methods in Natural\\nLanguage Processing , pp. 4553–4569, 2022.\\nPolya, G. How to solve it: A new aspect of mathematical\\nmethod , volume 85. Princeton university press, 2004.\\nRasmussen, J. Skills, rules, and knowledge; signals, signs,\\nand symbols, and other distinctions in human perfor-\\nmance models. IEEE transactions on systems, man, and\\ncybernetics , (3):257–266, 1983.Saha, S., Levy, O., Celikyilmaz, A., Bansal, M., Weston,\\nJ., and Li, X. Branch-solve-merge improves large lan-\\nguage model evaluation and generation. arXiv preprint\\narXiv:2310.15123 , 2023.\\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\\nGarriga-Alonso, A., et al. Beyond the imitation game:\\nQuantifying and extrapolating the capabilities of language\\nmodels. Transactions on Machine Learning Research ,\\n2023.\\nSuzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay,\\nY ., Chung, H. W., Chowdhery, A., Le, Q. V ., Chi,\\nE. H., Zhou, D., et al. Challenging big-bench tasks and\\nwhether chain-of-thought can solve them. arXiv preprint\\narXiv:2210.09261 , 2022.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundation and fine-\\ntuned chat models. arXiv preprint arXiv:2307.09288 ,\\n2023.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-\\ntion is all you need. In Advances in Neural Information\\nProcessing Systems , volume 30. Curran Associates, Inc.,\\n2017. URL https://proceedings.neurips.\\ncc/paper_files/paper/2017/file/\\n3f5ee243547dee91fbd053c1c4a845aa-Paper.\\npdf.\\nWang, L., Xu, W., Lan, Y ., Hu, Z., Lan, Y ., Lee, R. K.-W.,\\nand Lim, E.-P. Plan-and-solve prompting: Improving\\nzero-shot chain-of-thought reasoning by large language\\nmodels. arXiv preprint arXiv:2305.04091 , 2023.\\nWang, X., Wei, J., Schuurmans, D., Le, Q. V ., Chi,\\nE. H., Narang, S., Chowdhery, A., and Zhou, D. Self-\\nconsistency improves chain of thought reasoning in lan-'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_retriever(\"Architecture of BART neural network for natural language processing\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec156518-95d8-40cd-ba3f-b980158efd32",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Write a search query that will help answer a complex question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: Contexts produced from previous search queries.\n",
      "\n",
      "Question: The complex question we began with.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the query}. We ...\n",
      "\n",
      "Query: A search query that will help answer the question.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\n",
      "Krueger, Kevin Button, Matthew Knight, Benjamin\n",
      "Chess, and John Schulman. 2022. Webgpt: Browser-\n",
      "assisted question-answering with human feedback.\n",
      "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\n",
      "Carroll Wainwright, Pamela Mishkin, Chong Zhang,\n",
      "Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n",
      "2022. Training language models to follow instruc-\n",
      "tions with human feedback. Advances in Neural\n",
      "Information Processing Systems , 35:27730–27744.\n",
      "Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\n",
      "Tool augmented language models.\n",
      "John V Pavlik. 2023. Collaborating with chatgpt: Con-\n",
      "sidering the implications of generative artificial intel-\n",
      "ligence for journalism and media education. Journal-\n",
      "ism & Mass Communication Educator , 78(1):84–93.\n",
      "Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari,\n",
      "Gustavo Soares, Christopher Meek, and Sumit Gul-\n",
      "wani. 2022. Synchromesh: Reliable code generation\n",
      "from pre-trained language models. In International\n",
      "Conference on Learning Representations .\n",
      "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\n",
      "Noah Smith, and Mike Lewis. 2023. Measuring and\n",
      "narrowing the compositionality gap in language mod-\n",
      "els. In Findings of the Association for Computational\n",
      "Linguistics: EMNLP 2023 , pages 5687–5711, Singa-\n",
      "pore. Association for Computational Linguistics.\n",
      "Peng Qi, Yuhao Zhang, and Christopher D. Manning.\n",
      "2020. Stay hungry, stay focused: Generating infor-\n",
      "mative and specific questions in information-seeking\n",
      "conversations. In Findings of the Association for\n",
      "Computational Linguistics: EMNLP 2020 , pages 25–\n",
      "40, Online. Association for Computational Linguis-\n",
      "tics.»\n",
      "[2] «Wang, X., Wei, J., Schuurmans, D., Le, Q. V ., Chi,\n",
      "E. H., Narang, S., Chowdhery, A., and Zhou, D. Self-\n",
      "consistency improves chain of thought reasoning in lan-\n",
      "guage models. In The Eleventh International Conference\n",
      "on Learning Representations , 2022.\n",
      "Wei, J., Bosma, M., Zhao, V ., Guu, K., Yu, A. W., Lester,\n",
      "B., Du, N., Dai, A. M., and Le, Q. V . Finetuned lan-\n",
      "guage models are zero-shot learners. In International\n",
      "Conference on Learning Representations , 2021.\n",
      "Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\n",
      "Chi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought\n",
      "prompting elicits reasoning in large language models.\n",
      "Advances in Neural Information Processing Systems , 35:\n",
      "24824–24837, 2022.\n",
      "Yang, C., Wang, X., Lu, Y ., Liu, H., Le, Q. V ., Zhou, D., and\n",
      "Chen, X. Large language models as optimizers. arXiv\n",
      "preprint arXiv:2309.03409 , 2023.\n",
      "10»\n",
      "[3] «Published as a conference paper at ICLR 2024\n",
      "Question Answering with Long Input Texts, Yes! In Proceedings of the 2022 Conference of\n",
      "the North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies , pp. 5336–5358, Seattle, United States, July 2022. Association for Computational\n",
      "Linguistics. URL https://aclanthology.org/2022.naacl-main.391 .\n",
      "Fabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\n",
      "and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 ,\n",
      "2019. URL https://arxiv.org/abs/1909.01066 .\n",
      "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\n",
      "Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\n",
      "Methods, Analysis & Insights from Training Gopher. arXiv preprint arXiv:2112.11446 , 2021.\n",
      "URL https://arxiv.org/abs/2112.11446 .\n",
      "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-\n",
      "Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint\n",
      "arXiv:2302.00083 , 2023. URL https://arxiv.org/abs/2302.00083 .\n",
      "Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\n",
      "networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing and the 9th International Joint Conference on Natural Language Processing\n",
      "(EMNLP-IJCNLP) , pp. 3982–3992, Hong Kong, China, November 2019. Association for Com-\n",
      "putational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/\n",
      "D19-1410 .\n",
      "Adam Roberts, Colin Raffel, and Noam Shazeer. How Much Knowledge Can You Pack Into\n",
      "the Parameters of a Language Model? In Proceedings of the 2020 Conference on Empir-\n",
      "ical Methods in Natural Language Processing (EMNLP) , pp. 5418–5426, Online, November\n",
      "2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL\n",
      "https://aclanthology.org/2020.emnlp-main.437 .\n",
      "Stephen Robertson, Hugo Zaragoza, et al. The Probabilistic Relevance Framework: BM25 and\n",
      "Beyond. Foundations and Trends in Information Retrieval , 3(4):333–389, 2009. URL https:\n",
      "//doi.org/10.1561/1500000019 .\n",
      "Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,\n",
      "et al. Okapi at TREC-3. Nist Special Publication Sp , 109:109, 1995. URL https://www.\n",
      "microsoft.com/en-us/research/publication/okapi-at-trec-3/ .\n",
      "Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\n",
      "Zaheer. Questions are all you need to train a dense passage retriever. Transactions of the As-\n",
      "sociation for Computational Linguistics , 11:600–616, 2023. doi: 10.1162/tacl a00564. URL\n",
      "https://aclanthology.org/2023.tacl-1.35 .\n",
      "Gideon Schwarz. Estimating the Dimension of a Model. The annals of statistics , pp. 461–464,\n",
      "1978. URL https://projecteuclid.org/journals/annals-of-statistics/\n",
      "volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/\n",
      "aos/1176344136.full .\n",
      "Karen Sp ¨arck Jones. A Statistical Interpretation of Term Specificity and its Application in Re-\n",
      "trieval. Journal of documentation , 28(1):11–21, 1972. URL https://doi.org/10.1108/\n",
      "eb026526 .\n",
      "Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\n",
      "models actually use long-range context? In Marie-Francine Moens, Xuanjing Huang, Lucia\n",
      "Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods\n",
      "in Natural Language Processing , pp. 807–822, Online and Punta Cana, Dominican Republic,\n",
      "November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n",
      "62. URL https://aclanthology.org/2021.emnlp-main.62 .\n",
      "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\n",
      "models. arXiv preprint arXiv:2210.01296 , 2022. URL https://arxiv.org/abs/2210.\n",
      "01296 .\n",
      "14»\n",
      "\n",
      "Question: What kind of neural network is BART?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the query. BART is a specific type of neural network model used for natural language processing tasks. To determine what kind of neural network BART is, we need to look for information on its architecture, design, and purpose. This information can typically be found in research papers, technical documentation, or articles discussing BART's development and implementation.\n",
      "\n",
      "Query: \"BART neural network architecture\"\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Write a search query that will help answer a complex question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: Contexts produced from previous search queries.\n",
      "\n",
      "Question: The complex question we began with.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the query}. We ...\n",
      "\n",
      "Query: A search query that will help answer the question.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\n",
      "Krueger, Kevin Button, Matthew Knight, Benjamin\n",
      "Chess, and John Schulman. 2022. Webgpt: Browser-\n",
      "assisted question-answering with human feedback.\n",
      "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\n",
      "Carroll Wainwright, Pamela Mishkin, Chong Zhang,\n",
      "Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n",
      "2022. Training language models to follow instruc-\n",
      "tions with human feedback. Advances in Neural\n",
      "Information Processing Systems , 35:27730–27744.\n",
      "Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\n",
      "Tool augmented language models.\n",
      "John V Pavlik. 2023. Collaborating with chatgpt: Con-\n",
      "sidering the implications of generative artificial intel-\n",
      "ligence for journalism and media education. Journal-\n",
      "ism & Mass Communication Educator , 78(1):84–93.\n",
      "Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari,\n",
      "Gustavo Soares, Christopher Meek, and Sumit Gul-\n",
      "wani. 2022. Synchromesh: Reliable code generation\n",
      "from pre-trained language models. In International\n",
      "Conference on Learning Representations .\n",
      "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\n",
      "Noah Smith, and Mike Lewis. 2023. Measuring and\n",
      "narrowing the compositionality gap in language mod-\n",
      "els. In Findings of the Association for Computational\n",
      "Linguistics: EMNLP 2023 , pages 5687–5711, Singa-\n",
      "pore. Association for Computational Linguistics.\n",
      "Peng Qi, Yuhao Zhang, and Christopher D. Manning.\n",
      "2020. Stay hungry, stay focused: Generating infor-\n",
      "mative and specific questions in information-seeking\n",
      "conversations. In Findings of the Association for\n",
      "Computational Linguistics: EMNLP 2020 , pages 25–\n",
      "40, Online. Association for Computational Linguis-\n",
      "tics.»\n",
      "[2] «Wang, X., Wei, J., Schuurmans, D., Le, Q. V ., Chi,\n",
      "E. H., Narang, S., Chowdhery, A., and Zhou, D. Self-\n",
      "consistency improves chain of thought reasoning in lan-\n",
      "guage models. In The Eleventh International Conference\n",
      "on Learning Representations , 2022.\n",
      "Wei, J., Bosma, M., Zhao, V ., Guu, K., Yu, A. W., Lester,\n",
      "B., Du, N., Dai, A. M., and Le, Q. V . Finetuned lan-\n",
      "guage models are zero-shot learners. In International\n",
      "Conference on Learning Representations , 2021.\n",
      "Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\n",
      "Chi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought\n",
      "prompting elicits reasoning in large language models.\n",
      "Advances in Neural Information Processing Systems , 35:\n",
      "24824–24837, 2022.\n",
      "Yang, C., Wang, X., Lu, Y ., Liu, H., Le, Q. V ., Zhou, D., and\n",
      "Chen, X. Large language models as optimizers. arXiv\n",
      "preprint arXiv:2309.03409 , 2023.\n",
      "10»\n",
      "[3] «Published as a conference paper at ICLR 2024\n",
      "Question Answering with Long Input Texts, Yes! In Proceedings of the 2022 Conference of\n",
      "the North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies , pp. 5336–5358, Seattle, United States, July 2022. Association for Computational\n",
      "Linguistics. URL https://aclanthology.org/2022.naacl-main.391 .\n",
      "Fabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\n",
      "and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 ,\n",
      "2019. URL https://arxiv.org/abs/1909.01066 .\n",
      "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\n",
      "Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\n",
      "Methods, Analysis & Insights from Training Gopher. arXiv preprint arXiv:2112.11446 , 2021.\n",
      "URL https://arxiv.org/abs/2112.11446 .\n",
      "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-\n",
      "Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint\n",
      "arXiv:2302.00083 , 2023. URL https://arxiv.org/abs/2302.00083 .\n",
      "Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\n",
      "networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing and the 9th International Joint Conference on Natural Language Processing\n",
      "(EMNLP-IJCNLP) , pp. 3982–3992, Hong Kong, China, November 2019. Association for Com-\n",
      "putational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/\n",
      "D19-1410 .\n",
      "Adam Roberts, Colin Raffel, and Noam Shazeer. How Much Knowledge Can You Pack Into\n",
      "the Parameters of a Language Model? In Proceedings of the 2020 Conference on Empir-\n",
      "ical Methods in Natural Language Processing (EMNLP) , pp. 5418–5426, Online, November\n",
      "2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL\n",
      "https://aclanthology.org/2020.emnlp-main.437 .\n",
      "Stephen Robertson, Hugo Zaragoza, et al. The Probabilistic Relevance Framework: BM25 and\n",
      "Beyond. Foundations and Trends in Information Retrieval , 3(4):333–389, 2009. URL https:\n",
      "//doi.org/10.1561/1500000019 .\n",
      "Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,\n",
      "et al. Okapi at TREC-3. Nist Special Publication Sp , 109:109, 1995. URL https://www.\n",
      "microsoft.com/en-us/research/publication/okapi-at-trec-3/ .\n",
      "Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\n",
      "Zaheer. Questions are all you need to train a dense passage retriever. Transactions of the As-\n",
      "sociation for Computational Linguistics , 11:600–616, 2023. doi: 10.1162/tacl a00564. URL\n",
      "https://aclanthology.org/2023.tacl-1.35 .\n",
      "Gideon Schwarz. Estimating the Dimension of a Model. The annals of statistics , pp. 461–464,\n",
      "1978. URL https://projecteuclid.org/journals/annals-of-statistics/\n",
      "volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/\n",
      "aos/1176344136.full .\n",
      "Karen Sp ¨arck Jones. A Statistical Interpretation of Term Specificity and its Application in Re-\n",
      "trieval. Journal of documentation , 28(1):11–21, 1972. URL https://doi.org/10.1108/\n",
      "eb026526 .\n",
      "Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\n",
      "models actually use long-range context? In Marie-Francine Moens, Xuanjing Huang, Lucia\n",
      "Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods\n",
      "in Natural Language Processing , pp. 807–822, Online and Punta Cana, Dominican Republic,\n",
      "November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n",
      "62. URL https://aclanthology.org/2021.emnlp-main.62 .\n",
      "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\n",
      "models. arXiv preprint arXiv:2210.01296 , 2022. URL https://arxiv.org/abs/2210.\n",
      "01296 .\n",
      "14»\n",
      "[4] «SELF-DISCOVER : Large Language Models Self-Compose Reasoning Structures\n",
      "Acknowledgement\n",
      "We thank Andrew Dai and Adams Yu of Google DeepMind\n",
      "for their insightful feedback on this paper.\n",
      "References\n",
      "Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\n",
      "D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\n",
      "Z., et al. Palm 2 technical report. arXiv preprint\n",
      "arXiv:2305.10403 , 2023.\n",
      "Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gi-\n",
      "aninazzi, L., Gajda, J., Lehmann, T., Podstawski, M.,\n",
      "Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts:\n",
      "Solving elaborate problems with large language models.\n",
      "arXiv preprint arXiv:2308.09687 , 2023.\n",
      "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\n",
      "Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\n",
      "Askell, A., et al. Language models are few-shot learners.\n",
      "Advances in neural information processing systems , 33:\n",
      "1877–1901, 2020.\n",
      "Chen, J., Pan, X., Yu, D., Song, K., Wang, X., Yu, D.,\n",
      "and Chen, J. Skills-in-context prompting: Unlocking\n",
      "compositionality in large language models. arXiv preprint\n",
      "arXiv:2308.00304 , 2023.\n",
      "Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program\n",
      "of thoughts prompting: Disentangling computation from\n",
      "reasoning for numerical reasoning tasks. arXiv preprint\n",
      "arXiv:2211.12588 , 2022.\n",
      "Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\n",
      "G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\n",
      "Gehrmann, S., et al. Palm: Scaling language modeling\n",
      "with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n",
      "Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\n",
      "Fedus, W., Li, Y ., Wang, X., Dehghani, M., Brahma,\n",
      "S., et al. Scaling instruction-finetuned language models.\n",
      "arXiv preprint arXiv:2210.11416 , 2022.\n",
      "Cobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H.,\n",
      "Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\n",
      "R., et al. Training verifiers to solve math word problems.\n",
      "arXiv preprint arXiv:2110.14168 , 2021.\n",
      "Drozdov, A., Schärli, N., Akyürek, E., Scales, N., Song,\n",
      "X., Chen, X., Bousquet, O., and Zhou, D. Composi-\n",
      "tional semantic parsing with large language models. arXiv\n",
      "preprint arXiv:2209.15003 , 2022.\n",
      "Fernando, C., Banarse, D., Michalewski, H., Osindero,\n",
      "S., and Rocktäschel, T. Promptbreeder: Self-referential\n",
      "self-improvement via prompt evolution. arXiv preprint\n",
      "arXiv:2309.16797 , 2023.Gao, C., Jiang, H., Cai, D., Shi, S., and Lam, W. Strategyllm:\n",
      "Large language models as strategy generators, executors,\n",
      "optimizers, and evaluators for problem solving. arXiv\n",
      "preprint arXiv:2311.08803 , 2023a.\n",
      "Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang,\n",
      "Y ., Callan, J., and Neubig, G. Pal: Program-aided lan-\n",
      "guage models. In International Conference on Machine\n",
      "Learning , pp. 10764–10799. PMLR, 2023b.\n",
      "Hao, S., Gu, Y ., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z.,\n",
      "and Hu, Z. Reasoning with language model is planning\n",
      "with world model. arXiv preprint arXiv:2305.14992 ,\n",
      "2023.\n",
      "Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\n",
      "S., Tang, E., Song, D., and Steinhardt, J. Measuring\n",
      "mathematical problem solving with the math dataset. Sort,\n",
      "2(4):0–6.\n",
      "Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\n",
      "S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\n",
      "ematical problem solving with the math dataset, 2021.\n",
      "Khot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\n",
      "Clark, P., and Sabharwal, A. Decomposed prompting:\n",
      "A modular approach for solving complex tasks. In The\n",
      "Eleventh International Conference on Learning Repre-\n",
      "sentations , 2022.\n",
      "Kojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\n",
      "Y . Large language models are zero-shot reasoners. Ad-\n",
      "vances in neural information processing systems , 35:\n",
      "22199–22213, 2022.\n",
      "Kuznia, K., Mishra, S., Parmar, M., and Baral, C. Less is\n",
      "more: Summary of long instructions is better for program\n",
      "synthesis. In Proceedings of the 2022 Conference on\n",
      "Empirical Methods in Natural Language Processing , pp.\n",
      "4532–4552, 2022.\n",
      "Liu, T., Guo, Q., Yang, Y ., Hu, X., Zhang, Y ., Qiu, X.,»\n",
      "\n",
      "Question: What kind of neural network is BART?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the query. BART is a type of neural network used for natural language processing tasks. To determine what kind of neural network BART is, we need to search for information on the architecture and design of BART.\n",
      "\n",
      "Query: \"BART neural network architecture and design\"\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: Helpful information for answering the question.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: A detailed answer that is supported by the context.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\n",
      "Krueger, Kevin Button, Matthew Knight, Benjamin\n",
      "Chess, and John Schulman. 2022. Webgpt: Browser-\n",
      "assisted question-answering with human feedback.\n",
      "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\n",
      "Carroll Wainwright, Pamela Mishkin, Chong Zhang,\n",
      "Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n",
      "2022. Training language models to follow instruc-\n",
      "tions with human feedback. Advances in Neural\n",
      "Information Processing Systems , 35:27730–27744.\n",
      "Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\n",
      "Tool augmented language models.\n",
      "John V Pavlik. 2023. Collaborating with chatgpt: Con-\n",
      "sidering the implications of generative artificial intel-\n",
      "ligence for journalism and media education. Journal-\n",
      "ism & Mass Communication Educator , 78(1):84–93.\n",
      "Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari,\n",
      "Gustavo Soares, Christopher Meek, and Sumit Gul-\n",
      "wani. 2022. Synchromesh: Reliable code generation\n",
      "from pre-trained language models. In International\n",
      "Conference on Learning Representations .\n",
      "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\n",
      "Noah Smith, and Mike Lewis. 2023. Measuring and\n",
      "narrowing the compositionality gap in language mod-\n",
      "els. In Findings of the Association for Computational\n",
      "Linguistics: EMNLP 2023 , pages 5687–5711, Singa-\n",
      "pore. Association for Computational Linguistics.\n",
      "Peng Qi, Yuhao Zhang, and Christopher D. Manning.\n",
      "2020. Stay hungry, stay focused: Generating infor-\n",
      "mative and specific questions in information-seeking\n",
      "conversations. In Findings of the Association for\n",
      "Computational Linguistics: EMNLP 2020 , pages 25–\n",
      "40, Online. Association for Computational Linguis-\n",
      "tics.»\n",
      "[2] «Wang, X., Wei, J., Schuurmans, D., Le, Q. V ., Chi,\n",
      "E. H., Narang, S., Chowdhery, A., and Zhou, D. Self-\n",
      "consistency improves chain of thought reasoning in lan-\n",
      "guage models. In The Eleventh International Conference\n",
      "on Learning Representations , 2022.\n",
      "Wei, J., Bosma, M., Zhao, V ., Guu, K., Yu, A. W., Lester,\n",
      "B., Du, N., Dai, A. M., and Le, Q. V . Finetuned lan-\n",
      "guage models are zero-shot learners. In International\n",
      "Conference on Learning Representations , 2021.\n",
      "Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\n",
      "Chi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought\n",
      "prompting elicits reasoning in large language models.\n",
      "Advances in Neural Information Processing Systems , 35:\n",
      "24824–24837, 2022.\n",
      "Yang, C., Wang, X., Lu, Y ., Liu, H., Le, Q. V ., Zhou, D., and\n",
      "Chen, X. Large language models as optimizers. arXiv\n",
      "preprint arXiv:2309.03409 , 2023.\n",
      "10»\n",
      "[3] «Published as a conference paper at ICLR 2024\n",
      "Question Answering with Long Input Texts, Yes! In Proceedings of the 2022 Conference of\n",
      "the North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies , pp. 5336–5358, Seattle, United States, July 2022. Association for Computational\n",
      "Linguistics. URL https://aclanthology.org/2022.naacl-main.391 .\n",
      "Fabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\n",
      "and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 ,\n",
      "2019. URL https://arxiv.org/abs/1909.01066 .\n",
      "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\n",
      "Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\n",
      "Methods, Analysis & Insights from Training Gopher. arXiv preprint arXiv:2112.11446 , 2021.\n",
      "URL https://arxiv.org/abs/2112.11446 .\n",
      "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-\n",
      "Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint\n",
      "arXiv:2302.00083 , 2023. URL https://arxiv.org/abs/2302.00083 .\n",
      "Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\n",
      "networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing and the 9th International Joint Conference on Natural Language Processing\n",
      "(EMNLP-IJCNLP) , pp. 3982–3992, Hong Kong, China, November 2019. Association for Com-\n",
      "putational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/\n",
      "D19-1410 .\n",
      "Adam Roberts, Colin Raffel, and Noam Shazeer. How Much Knowledge Can You Pack Into\n",
      "the Parameters of a Language Model? In Proceedings of the 2020 Conference on Empir-\n",
      "ical Methods in Natural Language Processing (EMNLP) , pp. 5418–5426, Online, November\n",
      "2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL\n",
      "https://aclanthology.org/2020.emnlp-main.437 .\n",
      "Stephen Robertson, Hugo Zaragoza, et al. The Probabilistic Relevance Framework: BM25 and\n",
      "Beyond. Foundations and Trends in Information Retrieval , 3(4):333–389, 2009. URL https:\n",
      "//doi.org/10.1561/1500000019 .\n",
      "Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,\n",
      "et al. Okapi at TREC-3. Nist Special Publication Sp , 109:109, 1995. URL https://www.\n",
      "microsoft.com/en-us/research/publication/okapi-at-trec-3/ .\n",
      "Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\n",
      "Zaheer. Questions are all you need to train a dense passage retriever. Transactions of the As-\n",
      "sociation for Computational Linguistics , 11:600–616, 2023. doi: 10.1162/tacl a00564. URL\n",
      "https://aclanthology.org/2023.tacl-1.35 .\n",
      "Gideon Schwarz. Estimating the Dimension of a Model. The annals of statistics , pp. 461–464,\n",
      "1978. URL https://projecteuclid.org/journals/annals-of-statistics/\n",
      "volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/\n",
      "aos/1176344136.full .\n",
      "Karen Sp ¨arck Jones. A Statistical Interpretation of Term Specificity and its Application in Re-\n",
      "trieval. Journal of documentation , 28(1):11–21, 1972. URL https://doi.org/10.1108/\n",
      "eb026526 .\n",
      "Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\n",
      "models actually use long-range context? In Marie-Francine Moens, Xuanjing Huang, Lucia\n",
      "Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods\n",
      "in Natural Language Processing , pp. 807–822, Online and Punta Cana, Dominican Republic,\n",
      "November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n",
      "62. URL https://aclanthology.org/2021.emnlp-main.62 .\n",
      "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\n",
      "models. arXiv preprint arXiv:2210.01296 , 2022. URL https://arxiv.org/abs/2210.\n",
      "01296 .\n",
      "14»\n",
      "[4] «SELF-DISCOVER : Large Language Models Self-Compose Reasoning Structures\n",
      "Acknowledgement\n",
      "We thank Andrew Dai and Adams Yu of Google DeepMind\n",
      "for their insightful feedback on this paper.\n",
      "References\n",
      "Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\n",
      "D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\n",
      "Z., et al. Palm 2 technical report. arXiv preprint\n",
      "arXiv:2305.10403 , 2023.\n",
      "Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gi-\n",
      "aninazzi, L., Gajda, J., Lehmann, T., Podstawski, M.,\n",
      "Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts:\n",
      "Solving elaborate problems with large language models.\n",
      "arXiv preprint arXiv:2308.09687 , 2023.\n",
      "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\n",
      "Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\n",
      "Askell, A., et al. Language models are few-shot learners.\n",
      "Advances in neural information processing systems , 33:\n",
      "1877–1901, 2020.\n",
      "Chen, J., Pan, X., Yu, D., Song, K., Wang, X., Yu, D.,\n",
      "and Chen, J. Skills-in-context prompting: Unlocking\n",
      "compositionality in large language models. arXiv preprint\n",
      "arXiv:2308.00304 , 2023.\n",
      "Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program\n",
      "of thoughts prompting: Disentangling computation from\n",
      "reasoning for numerical reasoning tasks. arXiv preprint\n",
      "arXiv:2211.12588 , 2022.\n",
      "Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\n",
      "G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\n",
      "Gehrmann, S., et al. Palm: Scaling language modeling\n",
      "with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n",
      "Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\n",
      "Fedus, W., Li, Y ., Wang, X., Dehghani, M., Brahma,\n",
      "S., et al. Scaling instruction-finetuned language models.\n",
      "arXiv preprint arXiv:2210.11416 , 2022.\n",
      "Cobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H.,\n",
      "Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\n",
      "R., et al. Training verifiers to solve math word problems.\n",
      "arXiv preprint arXiv:2110.14168 , 2021.\n",
      "Drozdov, A., Schärli, N., Akyürek, E., Scales, N., Song,\n",
      "X., Chen, X., Bousquet, O., and Zhou, D. Composi-\n",
      "tional semantic parsing with large language models. arXiv\n",
      "preprint arXiv:2209.15003 , 2022.\n",
      "Fernando, C., Banarse, D., Michalewski, H., Osindero,\n",
      "S., and Rocktäschel, T. Promptbreeder: Self-referential\n",
      "self-improvement via prompt evolution. arXiv preprint\n",
      "arXiv:2309.16797 , 2023.Gao, C., Jiang, H., Cai, D., Shi, S., and Lam, W. Strategyllm:\n",
      "Large language models as strategy generators, executors,\n",
      "optimizers, and evaluators for problem solving. arXiv\n",
      "preprint arXiv:2311.08803 , 2023a.\n",
      "Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang,\n",
      "Y ., Callan, J., and Neubig, G. Pal: Program-aided lan-\n",
      "guage models. In International Conference on Machine\n",
      "Learning , pp. 10764–10799. PMLR, 2023b.\n",
      "Hao, S., Gu, Y ., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z.,\n",
      "and Hu, Z. Reasoning with language model is planning\n",
      "with world model. arXiv preprint arXiv:2305.14992 ,\n",
      "2023.\n",
      "Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\n",
      "S., Tang, E., Song, D., and Steinhardt, J. Measuring\n",
      "mathematical problem solving with the math dataset. Sort,\n",
      "2(4):0–6.\n",
      "Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\n",
      "S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\n",
      "ematical problem solving with the math dataset, 2021.\n",
      "Khot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\n",
      "Clark, P., and Sabharwal, A. Decomposed prompting:\n",
      "A modular approach for solving complex tasks. In The\n",
      "Eleventh International Conference on Learning Repre-\n",
      "sentations , 2022.\n",
      "Kojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\n",
      "Y . Large language models are zero-shot reasoners. Ad-\n",
      "vances in neural information processing systems , 35:\n",
      "22199–22213, 2022.\n",
      "Kuznia, K., Mishra, S., Parmar, M., and Baral, C. Less is\n",
      "more: Summary of long instructions is better for program\n",
      "synthesis. In Proceedings of the 2022 Conference on\n",
      "Empirical Methods in Natural Language Processing , pp.\n",
      "4532–4552, 2022.\n",
      "Liu, T., Guo, Q., Yang, Y ., Hu, X., Zhang, Y ., Qiu, X.,»\n",
      "\n",
      "Question: What kind of neural network is BART?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m determine the type of neural network BART is. We need to refer to the context provided to find information related to BART and its classification.\n",
      "\n",
      "Answer: BART is a type of neural network known as a sequence-to-sequence model.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chatgpt.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de5290-fa58-43d2-8fc9-7df907e86274",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/kuba/.cache/pypoetry/virtualenvs/llms-dspy-cWHDaHg3-py3.10/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "dspy",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "dspy"
  },
  "name": "DSPy_PDFs.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
