{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65152e0-3ca3-4fc1-bae5-3d8c2869c057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.cache/pypoetry/virtualenvs/llms-dspy-cWHDaHg3-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "import dspy\n",
    "import dspy.retrieve\n",
    "from llms_dspy.dspy_modules import SimpleRAG\n",
    "from llms_dspy.utils import get_llm, get_qdrant_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f63f54-abfc-47ef-b417-abb245d36df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    retriever = get_qdrant_retriever(collection_name=\"org\")\n",
    "except BlockingIOError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1012aa27-92aa-43ee-ab73-0d28f47f13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_type = \"vllm\"\n",
    "\n",
    "llm = get_llm(llm_type)\n",
    "dspy.settings.configure(lm=llm, rm=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "545f6cd6-9686-4f86-a3dc-64b959100ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What libraries have similar functionality to Langchain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bfa3e96-29a5-4d96-a03c-d949847b9c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the libraries that Langchain uses, we can recommend the following libraries that have similar functionality:\\n\\n* Chroma: A library for storing and querying vectors.\\n* DSPy: A library for data processing.\\n* Hugging Face Transformers: A library for natural language processing that includes pre-trained models for tasks such as text classification and language translation.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag = SimpleRAG(num_passages=10)  # zero-shot, uncompiled version of RAG\n",
    "context, response = rag(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9b07c18-b608-419d-a8ad-54cb5b64c577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the libraries that Langchain uses, we can recommend the following libraries that have similar functionality:\n",
      "\n",
      "* Chroma: A library for storing and querying vectors.\n",
      "* DSPy: A library for data processing.\n",
      "* Hugging Face Transformers: A library for natural language processing that includes pre-trained models for tasks such as text classification and language translation.\n"
     ]
    }
   ],
   "source": [
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4886bb82-2ffa-4dec-a205-bf6293d70b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'find libraries with similar functionality to Langchain. We can start by looking at the libraries that Langchain uses. Langchain uses Chroma as a vector store, which is a library for storing and querying vectors. We can also see that Langchain uses DSPy, which is a library for data processing.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "467716ac-537d-49dd-8f88-a3484a55f845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llms/langchain emacs\\n\\norg-roam & langchain\\n\\nDONE local file integration\\n\\nflagship example has notion dump with markdown files\\n\\nlangchain + ChatGPT\\n\\nlangchain + lokalny LLM',\n",
       " 'langchainexpressionlanguage\\n\\ndspy',\n",
       " 'from langchain.vectorstores import Chroma\\n\\nimport indexing\\n\\nembedding = indexing.EmbeddingConfig.get_default().load_embeddings()\\n\\nembedding\\n\\n#db = Chroma(persist_directory=\"vectordb\",embedding_function=embedding, collection_name=\"langchain_rtdocs\")\\n\\nimport chromadb\\n\\nchroma_settings = chromadb.config.Settings(persist_directory=\"./vectordb\")\\n\\nclient = chromadb.Client(chroma_settings)\\n\\nclient.list_collections()',\n",
       " 'llms/langchain\\n\\nRag example from fullstack retrieval\\n\\nTop k search uses this\\n\\nfrom langchain import hub\\n\\nfrom langchain.chat_models import ChatOpenAI\\n\\nfrom langchain.schema import StrOutputParser\\n\\nfrom langchain.schema.runnable import RunnablePassthrough\\n\\nretriever = vectorstore.as_retriever()\\n\\nrag_chain = (\\n\\n{\"context\": retriever, \"question\": RunnablePassthrough()}\\n\\n| prompt\\n\\n| llms\\n\\n| StrOutputParser()',\n",
       " 'llmdocs\\n\\nexact commit\\n\\nDocument loading & preprocessing\\n\\nLangchain has several connectors.\\n\\nPreprocessors can be configured to extract document chunks\\n\\nEmbedding\\n\\nDocuments can be encoded with a huggingface model or using an external API.\\n\\nSearch\\n\\nLibrary connects to vector databases like Qdrant, Chroma et c.\\n\\nAnswering\\n\\nBased on retrieved documents a separate component runs LM to generate answer.',\n",
       " 'langchain prompts',\n",
       " 'trash\\n\\nTL; DR\\n\\nToolkit for stuff useful for writing gpt-based extensions integrate with components like langchain et c.\\n\\nUse cases\\n\\ngolemacs/refactoring\\n\\ngolemacs/description\\n\\ngolemacs/importinference\\n\\ncode Q&A\\n\\naugmented search with semantic search',\n",
       " 'related to llamaindex\\n\\nTODO org & langchain\\n\\nDONE local file integration\\n\\nflagship example has notion dump with markdown files\\n\\nTODO langchain + embeddings with own model\\n\\nhuggingface embeddings\\n\\n[ ] put huggingface model in flagship example\\n\\nTODO langchain + ChatGPT\\n\\nTODO langchain + lokalny LLM\\n\\ncode & langchain',\n",
       " 'notebooks\\n\\n20231208104827-programmingnotebooks.org::7 (in /home/kuba/Projects/org/roam/20231208104827-programmingnotebooks.org)\\n\\nLangchain is NOT for production use. Here is why ..\\n\\nnice medium article\\n\\nSecurity - why is this even here?\\n\\nLangchain has no such features, so in any case user would need to do this himself\\n\\nDSPy\\n\\nWnioski',\n",
       " 'TL;DR\\n\\nFullStackRetrieval.com by Greg Kamradt.\\n\\nThis site is a course introduction to RAG with examples in mostly LangChain - it is a principled approach, not just a langchain tutorial.\\n\\nComponents\\n\\nTODO retrievalquerytransformation\\n\\nDONE retrievalmultivector\\n\\nRetrieval methods\\n\\nllamaindex vs langchain\\n\\nTop k search\\n\\nInteresting example - uses LangSmith and langchainexpressionlanguage\\n\\nDONE maximummarginalrelevance\\n\\nLangchain\\nretriever_mmr = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_search_dspy",
   "language": "python",
   "name": "llm_search_dspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
