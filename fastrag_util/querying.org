#+title: Querying


#+BEGIN_SRC python :session querying.org  :exports both
from search_client import SearchClient, ResultEvaluator
import tqdm
import pandas as pd
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session querying.org  :exports both
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session querying.org  :exports both
queries = ['general classification',
 'image classification',
 'semantic segmentation',
 'object detection',
 'translation',
 'reinforcement learning',
 'classification',
 'language modelling',
 'representation learning',
 'question answering',
 'machine translation',
 'transfer learning',
 'image generation',
 'sentiment analysis',
 'frame',
 'data augmentation',
 'time series',
 'text classification',
 'domain adaptation',
 'super resolution',
 'pose estimation',
 'natural language inference',
 'real time object detection',
 'instance segmentation',
 'decision making',
 'person re identification',
 'named entity recognition',
 'image to image translation',
 'word embeddings',
 'denoising',
 'text generation',
 'style transfer',
 'recommendation systems',
 'natural language understanding',
 'self supervised learning',
 'speech recognition',
 'meta learning',
 'multi task learning',
 'text summarization',
 'medical image segmentation',
 'contrastive learning',
 'neural architecture search',
 'action recognition',
 'autonomous driving',
 'image super resolution',
 'abstractive text summarization',
 'variational inference',
 'anomaly detection',
 'q learning',
 'link prediction'
]


#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session querying.org  :exports both :async
def run_evaluation(api_url, id_field="repo_name", query_field="tasks"):
    searcher = SearchClient(api_url=api_url)
    query_results = {
        q: searcher.search_grouped(query=q, group_by=id_field, raw_docs_topk=5000)
            for q in tqdm.tqdm(queries)
    }

    evaluator = ResultEvaluator(result_key=id_field, true_queries_key=query_field)
    evaluation_results_df = pd.DataFrame.from_records([
        {query_field: query, **evaluator.get_metrics(results, query)}
        for (query, results) in query_results.items()
    ])
    return evaluation_results_df
#+END_SRC

#+RESULTS:

* READMEs

This requires setting up the index with

```
poetry run python plaid_app.py -c conf/index_readme_config.yaml
```

#+BEGIN_SRC python :session querying.org  :exports both :async
readme_evaluation_df = run_evaluation('http://localhost:4321', "title")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session querying.org  :exports both :async
readme_evaluation_df.drop(columns=["tasks"]).mean()
#+END_SRC

#+RESULTS:
: hits@10        6.80
: accuracy@10    0.94
: dtype: float64

* Code (sample)

run

```
poetry run python plaid_app.py -c conf/index_code_config.yaml -p 5432
```

#+BEGIN_SRC python :session querying.org  :exports both :async
code_evaluation_df = run_evaluation('http://localhost:5432', "repo_name")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session querying.org  :exports both :async
code_evaluation_df.drop(columns=["tasks"]).mean()
#+END_SRC

#+RESULTS:
: hits@10        4.62
: accuracy@10    0.90
: dtype: float64
